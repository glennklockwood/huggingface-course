{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9433631896972656}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9598051905632019}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"
     ]
    }
   ],
   "source": [
    "classifier = transformers.pipeline('sentiment-analysis')\n",
    "print(classifier(\"I have been waiting for a HuggingFace course my whole life.\"))\n",
    "print(classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transformers library', 'labels': ['hpc', 'ai', 'education', 'programming', 'business', 'politics'], 'scores': [0.3120686709880829, 0.29162275791168213, 0.27291351556777954, 0.07318029552698135, 0.0361822284758091, 0.014032515697181225]}\n"
     ]
    }
   ],
   "source": [
    "classifier = transformers.pipeline(\"zero-shot-classification\")\n",
    "print(classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"programming\", \"business\", \"ai\", \"hpc\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to build complex '\n",
      "                    'systems using your computer to achieve success. It will '\n",
      "                    'also give you tips on how to improve'},\n",
      " {'generated_text': 'In this course, we will teach you how to identify the '\n",
      "                    'difference in the two sexes in your home-based '\n",
      "                    'environment. In this study, we show'}]\n"
     ]
    }
   ],
   "source": [
    "generator = transformers.pipeline(\"text-generation\")\n",
    "pprint.pprint(generator(\"In this course, we will teach you how to\", num_return_sequences=2, max_length=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'My name is Glenn Lockwood and I am a veteran of three '\n",
      "                    'tours around the U.K. and Germany. I spent many years in '\n",
      "                    'the infantry between the ages of 14,15, and 16. I spent '\n",
      "                    '18 months in service and I won'},\n",
      " {'generated_text': 'My name is Glenn Lockwood and I am a small business owner '\n",
      "                    'and owner of my own office at 1430 West Fifth Avenue in '\n",
      "                    'New York City. As first lady of the United States I '\n",
      "                    'served on the Supreme Court of the United States of '\n",
      "                    'America ('}]\n"
     ]
    }
   ],
   "source": [
    "generator = transformers.pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "pprint.pprint(generator(\"My name is Glenn Lockwood and I am a\", num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.19619710743427277,\n",
      "  'sequence': 'This course will teach you all about mathematical models.',\n",
      "  'token': 30412,\n",
      "  'token_str': ' mathematical'},\n",
      " {'score': 0.04052681103348732,\n",
      "  'sequence': 'This course will teach you all about computational models.',\n",
      "  'token': 38163,\n",
      "  'token_str': ' computational'}]\n"
     ]
    }
   ],
   "source": [
    "unmasker = transformers.pipeline(\"fill-mask\")\n",
    "pprint.pprint(unmasker(\"This course will teach you all about <mask> models.\", top_k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/glock/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 18,\n",
      "  'entity_group': 'PER',\n",
      "  'score': 0.9981694,\n",
      "  'start': 11,\n",
      "  'word': 'Sylvain'},\n",
      " {'end': 45,\n",
      "  'entity_group': 'ORG',\n",
      "  'score': 0.97960204,\n",
      "  'start': 33,\n",
      "  'word': 'Hugging Face'},\n",
      " {'end': 57,\n",
      "  'entity_group': 'LOC',\n",
      "  'score': 0.9932106,\n",
      "  'start': 49,\n",
      "  'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "ner = transformers.pipeline(\"ner\", grouped_entities=True)\n",
    "pprint.pprint(ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Hugging Face', 'end': 45, 'score': 0.6949760913848877, 'start': 33}\n"
     ]
    }
   ],
   "source": [
    "question_answerer = transformers.pipeline(\"question-answering\")\n",
    "pprint.pprint(question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . China and India graduate six and eight times as many traditional engineers as does the United States .\n"
     ]
    }
   ],
   "source": [
    "LONG_TEXT = \"\"\"\n",
    "America has changed dramatically during recent years. Not only has the number of \n",
    "graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "the premier American universities engineering curricula now concentrate on \n",
    "and encourage largely the study of engineering science. As a result, there \n",
    "are declining offerings in engineering subjects dealing with infrastructure, \n",
    "the environment, and related issues, and greater concentration on high \n",
    "technology subjects, largely supporting increasingly complex scientific \n",
    "developments. While the latter is important, it should not be at the expense \n",
    "of more traditional engineering.\n",
    "\n",
    "Rapidly developing economies such as China and India, as well as other \n",
    "industrial countries in Europe and Asia, continue to encourage and advance \n",
    "the teaching of engineering. Both China and India, respectively, graduate \n",
    "six and eight times as many traditional engineers as does the United States. \n",
    "Other industrial countries at minimum maintain their output, while America \n",
    "suffers an increasingly serious decline in the number of engineering graduates \n",
    "and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    "summarizer = transformers.pipeline(\"summarization\")\n",
    "print(summarizer(LONG_TEXT)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man         :    carpenter(0.075)       lawyer(0.046)       farmer(0.039)  businessman(0.033)       doctor(0.029)\n",
      "woman       :        nurse(0.128)         maid(0.075)      teacher(0.072)     waitress(0.061)   prostitute(0.042)\n",
      "boy         :      teacher(0.112)    carpenter(0.074)     mechanic(0.060)       farmer(0.045)       waiter(0.038)\n",
      "girl        :     waitress(0.176)        nurse(0.131)        model(0.100)         maid(0.075)      teacher(0.059)\n"
     ]
    }
   ],
   "source": [
    "unmasker = transformers.pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "for loaded_word in [\"man\", \"woman\", \"boy\", \"girl\"]:\n",
    "    result = unmasker(f\"This {loaded_word} works as a [MASK].\")\n",
    "    print(\"{:12s}: {}\".format(\n",
    "        loaded_word,\n",
    "        \" \".join([\"{:>12s}({:.3f})\".format(r[\"token_str\"], r[\"score\"]) for r in result])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
